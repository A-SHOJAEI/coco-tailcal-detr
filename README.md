# coco-tailcal-detr

TailCal-DETR is a two-stage recipe to improve long-tail classification and confidence calibration for DETR-style object detectors:
1. Train a baseline detector.
2. Run a lightweight "TailCal" stage that (optionally) class-balances sampling, applies logit adjustment from class priors, and fits a post-hoc temperature for score calibration.

This repo includes an end-to-end pipeline that produces `artifacts/results.json` (machine-readable) and `artifacts/report.md` (the exact tables referenced below).

## Problem Statement

DETR-style detectors can be miscalibrated (overconfident false positives) and can underperform on rare categories. This project tests whether a small, targeted fine-tune of the classification head plus simple calibration techniques can improve:
- COCO detection metrics (mAP/AP50/AP75 via COCOeval)
- confidence calibration (ECE and AURC on COCO-format predictions)
- grouped AP by train-frequency buckets (tail/medium/head)

## Dataset Provenance

Two data modes exist (selected by `project.mode` in the YAML config):

- **Smoke mode (used for the included results)**: a tiny synthetic COCO-style dataset generated on the fly at `data/smoke_coco/` by `tailcal_detr/data/smoke_coco.py`. It writes COCO JSONs:
  - `data/smoke_coco/annotations/instances_train.json`
  - `data/smoke_coco/annotations/instances_val.json`

- **COCO mode (optional)**: downloads COCO 2017 zips listed in `configs/full.yaml` from `images.cocodataset.org` into `data/coco/downloads/`, verifies/download-caches them via `data/coco/downloads/manifest.sha256.json`, and extracts into `data/coco/` (`tailcal_detr/data/download_coco.py`).

This repo does not ship COCO images.

## Methodology (What The Code Actually Does)

Pipeline entrypoint: `tailcal_detr/pipeline.py` (invoked by `make`).

- **Baseline training** (`tailcal_detr/train.py`):
  - Builds a model via `tailcal_detr/model.py:build_detr_r50`.
  - Important implementation detail: it *tries* `torchvision.models.detection.detr_resnet50`, but with the pinned `torchvision==0.20.1` it falls back to a local CPU-friendly `TinyDETR` (50 queries, small transformer encoder).

- **Class priors + frequency buckets** (`tailcal_detr/priors.py`):
  - Computes per-category train counts and priors from the train annotation JSON.
  - Defines buckets by count quantiles: bottom 20% = tail, top 20% = head, rest = medium.
  - Saved to `artifacts/class_priors.json`.

- **TailCal fine-tuning** (`tailcal_detr/train_tailcal.py`):
  - Loads the baseline checkpoint, freezes everything, then unfreezes the classification head (`class_embed`).
  - Optional knobs (per experiment config):
    - **Balanced sampling**: `WeightedRandomSampler` with per-image weights computed from inverse class frequency of labels present.
    - **Logit adjustment**: wraps `class_embed` with `LogitAdjustedHead`, adding `tau * log(p_y)` to logits (and `0` for DETR's no-object class).
    - **Unfreeze last encoder block**: unfreezes `transformer.encoder.layers[-1]` when present.

- **Temperature scaling** (`tailcal_detr/temperature_scale.py` and `tailcal_detr/eval_coco.py`):
  - Fits a scalar temperature `T` by minimizing NLL over a deterministic calibration subset from the *start* of the training set (32 images in smoke, 5000 in COCO mode), with greedy IoU matching at 0.5.
  - Applies scaling at eval time by converting scores to logit space, dividing by `T`, and applying `sigmoid` (see `tailcal_detr/eval_coco.py:_score_temp_scale`).

- **Evaluation**:
  - COCO metrics via `pycocotools` COCOeval (`tailcal_detr/eval_coco.py`), writing `runs/<run>/metrics_coco.json` and `runs/<run>/preds_val.json`.
  - Calibration metrics (`tailcal_detr/eval_calibration.py`):
    - greedy per-(image,category) IoU matching at 0.5
    - ECE with 10 equal-width bins
    - AURC from the risk-coverage curve (trapezoidal integral)

## Baselines And Ablations (As Configured)

Defined in `configs/smoke.yaml` / `configs/full.yaml` as `experiments`:
- `baseline`: baseline training only.
- `tailcal_full`: balanced sampling + logit adjustment + temperature scaling (classification-head fine-tune).
- `ablate_balanced_only`: balanced sampling only (no logit adjustment, no temperature).
- `ablate_logit_only`: logit adjustment only (no balanced sampling, no temperature).
- `ablate_temp_only`: temperature scaling only (no TailCal training; the pipeline copies the baseline checkpoint into the run directory, then fits `T`).
- `ablate_unfreeze_transformer`: like `tailcal_full` plus unfreeze last transformer encoder block.

## Exact Results (From `artifacts/report.md`)

The current checked-in results were generated by running `make all` with `CONFIG=configs/smoke.yaml` (smoke mode) at `2026-02-20T06:07:34Z` and are summarized in `artifacts/report.md` (tables below are the source of truth).
`artifacts/report.md` currently contains tables only (no figures are generated by the pipeline).

### Table 1: COCO + Calibration Metrics

| Run | mAP | AP50 | AP75 | ECE | AURC | T |
| --- | ---:| ---:| ---:| ---:| ---:| ---:|
| baseline | 0.0000 | 0.0000 | 0.0000 | 0.2843 | 0.9950 | nan |
| tailcal_full | nan | nan | nan | nan | nan | 0.0506 |
| ablate_balanced_only | 0.0000 | 0.0000 | 0.0000 | 0.2819 | 0.9950 | nan |
| ablate_logit_only | 0.0000 | 0.0000 | 0.0000 | 0.1558 | 0.9947 | nan |
| ablate_temp_only | 0.0000 | 0.0000 | 0.0000 | 0.4714 | 0.9167 | 0.0881 |
| ablate_unfreeze_transformer | nan | nan | nan | nan | nan | 0.0506 |

### Table 2: Grouped AP (By Train Frequency Buckets)

| Run | Tail AP | Medium AP | Head AP |
| --- | ---:| ---:| ---:|
| baseline | 0.0000 | 0.0000 | 0.0000 |
| tailcal_full | nan | nan | nan |
| ablate_balanced_only | 0.0000 | 0.0000 | 0.0000 |
| ablate_logit_only | 0.0000 | 0.0000 | 0.0000 |
| ablate_temp_only | 0.0000 | 0.0000 | 0.0000 |
| ablate_unfreeze_transformer | nan | nan | nan |

Note: in smoke mode the class counts are small and uniform, so the head/medium/tail bucketing is not a realistic long-tail regime. The nan values for tailcal_full and ablate_unfreeze_transformer are due to 0 scored predictions passing calibration after fine-tuning on this tiny dataset.

## Repro (Exact Commands)

Smoke pipeline (generates the tables above):
```bash
make clean
make all
```

Full COCO pipeline (downloads COCO 2017 and runs the same experiment graph):
```bash
make all CONFIG=configs/full.yaml
```

Individual steps (all call `python -m tailcal_detr.pipeline <step> --config <yaml>`):
```bash
make data
make train
make eval
make report
```

Main outputs:
- `artifacts/results.json`: aggregated metrics + the exact resolved config used
- `artifacts/report.md`: the two result tables
- `runs/<experiment>/checkpoints/best.pt`: best checkpoint per run
- `runs/<experiment>/metrics_coco.json`, `runs/<experiment>/calibration.json`, `runs/<experiment>/preds_val.json`

## Limitations

- The pinned `torchvision==0.20.1` typically does not provide `detr_resnet50`, so this repo defaults to the local `TinyDETR` implementation; "DETR-R50" results are therefore not produced unless your environment provides that symbol.
- The included metrics are from **smoke mode** (8 train images, 4 val images, 1 epoch). They validate plumbing and ablations, not SOTA performance claims.
- Calibration metrics use greedy matching at IoU 0.5 and treat each predicted box independently; this is not the same as classification calibration for a closed-set classifier.
- Calibration `n` (the number of scored predictions used for ECE/AURC) depends on `eval.score_threshold` and `eval.top_k`, so ECE/AURC are not computed over a fixed-size sample across runs.
- Balanced sampling is disabled under DDP (the code falls back to `DistributedSampler`).

## Next Research Steps

1. Swap in a real DETR implementation (or a modern DETR variant) and rerun `configs/full.yaml` to measure TailCal effects on true COCO long-tail behavior.
2. Make the "tail" setting realistic: evaluate on naturally long-tail detection datasets (e.g., LVIS) or induce imbalance in COCO and re-measure grouped AP.
3. Replace greedy matching calibration with a calibration objective tied to COCOeval matching (or analyze calibration per class / per size / per score threshold).
4. Explore stronger head-only adaptations: class-balanced losses, focal loss variants, per-class temperature, or logit adjustment schedules.
